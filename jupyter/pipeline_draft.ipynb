{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Draft\n",
    "Pipeline functions to ingest, clean, interpolate, and normalize air pollutant concentration data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "AQ_FILE = \"Measurement_summary.csv\"\n",
    "WX_FILE = \"wx_file\"\n",
    "DIRI = %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation & Transformation Pipeline\n",
    "##### Takes file path, returns a dictionary with normalized and interpolated data weather station \n",
    "\n",
    "@param file: file name, defaults to above specified FILE  \n",
    "@param diri: directory with data file, defaults to above specified DIRI  \n",
    "@param compare: bool for comparing against a baseline set of stats, defaults False  \n",
    "@param verbose: bool for printing statistics, defaults False  \n",
    "@param interp: specify type of interpolation for missing values, defaults to akima\n",
    "\n",
    "@return station_dataframes: a dictionary with dataframes, key is TOOL_ID  \n",
    "\n",
    "##### Outline\n",
    "- Read data into CSV\n",
    "- Print Basic Statistics: mean, standard deviation, and skew by each machine\n",
    "- Data Characteristics: number outliers, number Nan values\n",
    "- Comparison: compare against saved characteristics of training data\n",
    "- Outliers: mask outliers (specified # std devs away from mean) with Nan, interpolate using an akima spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File that takes Xarray weather data as input and returns usable dataframe\n",
    "# Set variables to true if they need to be included in dataframe\n",
    "def xarray_conversion(dset, radiation=True, temperature=True, wind=False, pressure=False, precipitation=False):\n",
    "    # Column names in NetCFD\n",
    "    LATITUDE_NAME = \"latitude\"\n",
    "    LONGITUDE_NAME = \"longitude\"\n",
    "    TIME_NAME = \"time\"\n",
    "    TEMP_NAME = \"t2m\"\n",
    "    RAD_NAME = \"ssrd\"\n",
    "    PRESSURE_NAME = \"sp\"\n",
    "    PRECIP_NAME = \"tp\"\n",
    "    U_WIND_NAME = \"10u\"\n",
    "    V_WIND_NAME = \"10v\"\n",
    "\n",
    "    target_lat = dset[LATITUDE_NAME].median()\n",
    "    target_lon = dset[LONGITUDE_NAME].median()\n",
    "\n",
    "    dataset_lats = dset[LATITUDE_NAME]\n",
    "    dataset_lons = dset[LONGITUDE_NAME]\n",
    "\n",
    "    lat = dset[LATITUDE_NAME][(abs(dataset_lats - target_lat)).argmin()]\n",
    "    lon = dset[LONGITUDE_NAME][(abs(dataset_lons - target_lon)).argmin()]\n",
    "\n",
    "    wx_df = pd.DataFrame()\n",
    "\n",
    "    # Radiation reported in Joules/m^2/hour --> divide by seconds/hour to get Watts/m^2 instantaneous\n",
    "    wx_df[TIME_NAME] = dset[TIME_NAME]\n",
    "    if radiation:\n",
    "        wx_df[RAD_NAME] =  dset[RAD_NAME].sel(latitude=lat, longitude=lon) / 3600\n",
    "    if temperature:\n",
    "        wx_df[TEMP_NAME] = dset[TEMP_NAME].sel(latitude=lat, longitude=lon)\n",
    "    if pressure:\n",
    "        wx_df[PRESSURE_NAME] = dset[PRESSURE_NAME].sel(latitude=lat, longitude=lon)\n",
    "    if precipitation:\n",
    "        wx_df[PRECIP_NAME] = dset[PRECIP_NAME].sel(latitude=lat, longitude=lon)\n",
    "    if wind:\n",
    "        u_wind = dset[U_WIND_NAME].sel(latitude=lat, longitude=lon)\n",
    "        v_wind = dset[V_WIND_NAME].sel(latitude=lat, longitude=lon)\n",
    "        wx_df[\"wind\"] = np.sqrt(u_wind**2 + v_wind**2)\n",
    "\n",
    "    return wx_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_count(x, sigma=3):\n",
    "    stdev = x.std()\n",
    "    upper = x.mean() + sigma*stdev\n",
    "    lower = x.mean() - sigma*stdev\n",
    "\n",
    "    return len(x[(x > upper) | (x < lower)])\n",
    "\n",
    "\n",
    "def null_count(x):\n",
    "    return x.isnull().sum()\n",
    "\n",
    "def zscore(x):\n",
    "    return (x - x.mean())/x.std()\n",
    "\n",
    "def iqr(x):\n",
    "    return x.quantile(0.75) - x.quantile(0.25)\n",
    "\n",
    "def outlier_mask(x, sigma=3):\n",
    "    stdev = x.std()\n",
    "    upper = x.mean() + sigma*stdev\n",
    "    lower = x.mean() - sigma*stdev\n",
    "\n",
    "    return x.mask(((x > upper) | (x < lower)), np.nan).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Measurement date</th>\n",
       "      <th>Station code</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00</td>\n",
       "      <td>101</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>73.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 01:00</td>\n",
       "      <td>101</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>71.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 02:00</td>\n",
       "      <td>101</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 03:00</td>\n",
       "      <td>101</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 04:00</td>\n",
       "      <td>101</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>69.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647506</th>\n",
       "      <td>2019-12-31 19:00</td>\n",
       "      <td>125</td>\n",
       "      <td>37.544962</td>\n",
       "      <td>127.136792</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647507</th>\n",
       "      <td>2019-12-31 20:00</td>\n",
       "      <td>125</td>\n",
       "      <td>37.544962</td>\n",
       "      <td>127.136792</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647508</th>\n",
       "      <td>2019-12-31 21:00</td>\n",
       "      <td>125</td>\n",
       "      <td>37.544962</td>\n",
       "      <td>127.136792</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647509</th>\n",
       "      <td>2019-12-31 22:00</td>\n",
       "      <td>125</td>\n",
       "      <td>37.544962</td>\n",
       "      <td>127.136792</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647510</th>\n",
       "      <td>2019-12-31 23:00</td>\n",
       "      <td>125</td>\n",
       "      <td>37.544962</td>\n",
       "      <td>127.136792</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>647511 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Measurement date  Station code   Latitude   Longitude    SO2    NO2  \\\n",
       "0       2017-01-01 00:00           101  37.572016  127.005008  0.004  0.059   \n",
       "1       2017-01-01 01:00           101  37.572016  127.005008  0.004  0.058   \n",
       "2       2017-01-01 02:00           101  37.572016  127.005008  0.004  0.056   \n",
       "3       2017-01-01 03:00           101  37.572016  127.005008  0.004  0.056   \n",
       "4       2017-01-01 04:00           101  37.572016  127.005008  0.003  0.051   \n",
       "...                  ...           ...        ...         ...    ...    ...   \n",
       "647506  2019-12-31 19:00           125  37.544962  127.136792  0.003  0.028   \n",
       "647507  2019-12-31 20:00           125  37.544962  127.136792  0.003  0.025   \n",
       "647508  2019-12-31 21:00           125  37.544962  127.136792  0.003  0.023   \n",
       "647509  2019-12-31 22:00           125  37.544962  127.136792  0.003  0.040   \n",
       "647510  2019-12-31 23:00           125  37.544962  127.136792  0.003  0.037   \n",
       "\n",
       "           O3   CO  PM10  PM2.5  \n",
       "0       0.002  1.2  73.0   57.0  \n",
       "1       0.002  1.2  71.0   59.0  \n",
       "2       0.002  1.2  70.0   59.0  \n",
       "3       0.002  1.2  70.0   58.0  \n",
       "4       0.002  1.2  69.0   61.0  \n",
       "...       ...  ...   ...    ...  \n",
       "647506  0.013  0.5  23.0   17.0  \n",
       "647507  0.015  0.4  25.0   19.0  \n",
       "647508  0.015  0.4  24.0   17.0  \n",
       "647509  0.004  0.5  25.0   18.0  \n",
       "647510  0.005  0.5  27.0   18.0  \n",
       "\n",
       "[647511 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aq_path = os.path.join(\"..\", \"raw_data\", AQ_FILE)\n",
    "df = pd.read_csv(aq_path, na_values=-1)\n",
    "df.drop(\"Address\", axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Measurement date       0\n",
       "Station code           0\n",
       "Latitude               0\n",
       "Longitude              0\n",
       "SO2                 3976\n",
       "NO2                 3834\n",
       "O3                  4059\n",
       "CO                  4036\n",
       "PM10                3962\n",
       "PM2.5               3973\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validate_transform_pipeline(aq_file=AQ_FILE, wx_file=WX_FILE,\n",
    " diri=DIRI, verbose=True, interp=\"akima\"):\n",
    "    aq_file_path = os.path.join(diri, aq_file)\n",
    "    wx_file_path = os.path.join(diri, wx_file)\n",
    "\n",
    "    aq_df = pd.read_csv(aq_file_path)\n",
    "    wx_xr = xr.open_dataset(wx_file_path)\n",
    "    wx_df = xarray_conversion(wx_xr)\n",
    "\n",
    "    # Replace TOOL_ID numbers with more friendly integers\n",
    "    station_ids = aq_df[\"Station code\"].unique()\n",
    "    aq_df.replace(station_ids, range(len(station_ids)), inplace=True)\n",
    "\n",
    "    # All data is from SP5 in this dataset, remove column to strealine dataframe\n",
    "    aq_df.drop(\"Address\" axis=1, errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Store variable statistics in a dictionary\n",
    "    # Key: (Variable, Machine ID, Statistic) == number\n",
    "    # Statistics = [Mean, Standard Deviation, Skew Coefficient,\n",
    "    #               IQR, Outlier Count, Null Count, \n",
    "    #               Length of Dataset, Minimum, Maximum]\n",
    "\n",
    "    variables = ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5']\n",
    "\n",
    "    for var in variables:\n",
    "        aq_df.groupby(\"Station code\")[var].agg([np.mean, np.std,\n",
    "     sp.stats.skew, iqr, outlier_count, null_count, len, np.min, np.max])\n",
    "     \n",
    "    if verbose:\n",
    "        print(stats)\n",
    "\n",
    "    stats.to_csv(f\"Output/{file}_STATISTICS.csv\")\n",
    "\n",
    "    '''\n",
    "    if compare:\n",
    "        compare input statistics against statistics of baseline/training data\n",
    "    \n",
    "    '''\n",
    "    # Calculate Z-Scores for Variable values with statistics from same machine\n",
    "    df[\"z\"] = df.groupby([\"variable\", \"id\"])[\"value\"].transform(zscore)\n",
    "\n",
    "    # Convert input's time to a pandas datetime\n",
    "    # df[\"datetime\"] = pd.to_datetime(df[\"time\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    # Transform data to daily median values\n",
    "    df_daily_median = pd.DataFrame(df.groupby([\"date\", \"variable\",\n",
    "     \"id\"])[\"z\"].agg(np.median))\n",
    "\n",
    "    df_daily_median = df_daily_median.reset_index(level=\"variable\")\n",
    "    df_daily_median = df_daily_median.pivot(columns=\"variable\").reset_index()\n",
    "\n",
    "    # Adjusting columns after pivoting\n",
    "    df_daily_median = df_daily_median.transpose().reset_index(level=0, drop=True)\n",
    "    df_daily_median.reset_index(inplace=True)\n",
    "    df_daily_median[\"variable\"][0] = \"date\"\n",
    "    df_daily_median[\"variable\"][1] = \"id\"\n",
    "\n",
    "    df_daily_median = df_daily_median.transpose()\n",
    "    df_daily_median.columns = df_daily_median.iloc[0]\n",
    "    df_daily_median.drop(\"variable\", axis=0, inplace=True)\n",
    "    df_daily_median.drop(\"alias\", axis=1, inplace=True)\n",
    "\n",
    "    # Date column may have change types from transposing\n",
    "    # Reset as index for akima spline\n",
    "    df_daily_median[\"date\"] = pd.to_datetime(df_daily_median[\"date\"])\n",
    "    df_daily_median = df_daily_median.set_index(\"date\")\n",
    "\n",
    "    # All data represent as objects after transposing\n",
    "    # Reset to float values\n",
    "    for var in df_daily_median.columns:\n",
    "        df_daily_median[var] = pd.to_numeric(df_daily_median[var],errors = 'coerce')\n",
    "\n",
    "    # Remove \"target\" variables\n",
    "    target_cols = [col for col in df_daily_median.columns if 'target' in col]\n",
    "    df_daily_median.drop(target_cols, axis=1, inplace=True)\n",
    "\n",
    "    # key = id\n",
    "    machine_dataframes = {}\n",
    "    for machine_id in range(len(machine_ids)):\n",
    "        temp_df = df_daily_median.loc[\n",
    "            df_daily_median[\"id\"] == machine_id]\n",
    "\n",
    "        # Set all outliers to Nan values\n",
    "        outlier_df = outlier_mask(temp_df)\n",
    "\n",
    "        # Interpolate Nan values using akima spline\n",
    "        machine_dataframes[machine_id] = outlier_df.interpolate(method=interp)\n",
    "        # machine_dataframes[machine_id] = outlier_df\n",
    "\n",
    "    return machine_dataframes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3afd2e1adaba27579daa4edc92701dd83c620a06f8b35cec1bae252095baedd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
