{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Draft\n",
    "Pipeline functions to ingest, clean, interpolate, and normalize air pollutant concentration data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "AQ_FILE = \"Measurement_summary.csv\"\n",
    "WX_FILE = \"wx_file\"\n",
    "DIRI = %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation & Transformation Pipeline\n",
    "##### Takes file path, returns a dictionary with normalized and interpolated data weather station \n",
    "\n",
    "@param file: file name, defaults to above specified FILE  \n",
    "@param diri: directory with data file, defaults to above specified DIRI  \n",
    "@param compare: bool for comparing against a baseline set of stats, defaults False  \n",
    "@param verbose: bool for printing statistics, defaults False  \n",
    "@param interp: specify type of interpolation for missing values, defaults to akima\n",
    "\n",
    "@return station_dataframes: a dictionary with dataframes, key is TOOL_ID  \n",
    "\n",
    "##### Outline\n",
    "- Read data into CSV\n",
    "- Print Basic Statistics: mean, standard deviation, and skew by each machine\n",
    "- Data Characteristics: number outliers, number Nan values\n",
    "- Comparison: compare against saved characteristics of training data\n",
    "- Outliers: mask outliers (specified # std devs away from mean) with Nan, interpolate using an akima spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File that takes Xarray weather data as input and returns usable dataframe\n",
    "# Set variables to true if they need to be included in dataframe\n",
    "def xarray_conversion(dset, radiation=True, temperature=True, wind=False, pressure=False, precipitation=False):\n",
    "    # Column names in NetCFD\n",
    "    LATITUDE_NAME = \"latitude\"\n",
    "    LONGITUDE_NAME = \"longitude\"\n",
    "    TIME_NAME = \"time\"\n",
    "    TEMP_NAME = \"t2m\"\n",
    "    RAD_NAME = \"ssrd\"\n",
    "    PRESSURE_NAME = \"sp\"\n",
    "    PRECIP_NAME = \"tp\"\n",
    "    U_WIND_NAME = \"10u\"\n",
    "    V_WIND_NAME = \"10v\"\n",
    "\n",
    "    target_lat = dset[LATITUDE_NAME].median()\n",
    "    target_lon = dset[LONGITUDE_NAME].median()\n",
    "\n",
    "    dataset_lats = dset[LATITUDE_NAME]\n",
    "    dataset_lons = dset[LONGITUDE_NAME]\n",
    "\n",
    "    lat = dset[LATITUDE_NAME][(abs(dataset_lats - target_lat)).argmin()]\n",
    "    lon = dset[LONGITUDE_NAME][(abs(dataset_lons - target_lon)).argmin()]\n",
    "\n",
    "    wx_df = pd.DataFrame()\n",
    "\n",
    "    # Radiation reported in Joules/m^2/hour --> divide by seconds/hour to get Watts/m^2 instantaneous\n",
    "    wx_df[TIME_NAME] = dset[TIME_NAME]\n",
    "    if radiation:\n",
    "        wx_df[RAD_NAME] =  dset[RAD_NAME].sel(latitude=lat, longitude=lon) / 3600\n",
    "    if temperature:\n",
    "        wx_df[TEMP_NAME] = dset[TEMP_NAME].sel(latitude=lat, longitude=lon)\n",
    "    if pressure:\n",
    "        wx_df[PRESSURE_NAME] = dset[PRESSURE_NAME].sel(latitude=lat, longitude=lon)\n",
    "    if precipitation:\n",
    "        wx_df[PRECIP_NAME] = dset[PRECIP_NAME].sel(latitude=lat, longitude=lon)\n",
    "    if wind:\n",
    "        u_wind = dset[U_WIND_NAME].sel(latitude=lat, longitude=lon)\n",
    "        v_wind = dset[V_WIND_NAME].sel(latitude=lat, longitude=lon)\n",
    "        wx_df[\"wind\"] = np.sqrt(u_wind**2 + v_wind**2)\n",
    "\n",
    "    return wx_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_count(x, sigma=3):\n",
    "    stdev = x.std()\n",
    "    upper = x.mean() + sigma*stdev\n",
    "    lower = x.mean() - sigma*stdev\n",
    "\n",
    "    return len(x[(x > upper) | (x < lower)])\n",
    "\n",
    "\n",
    "def null_count(x):\n",
    "    return x.isnull().sum()\n",
    "\n",
    "def zscore(x):\n",
    "    return (x - x.mean())/x.std()\n",
    "\n",
    "def iqr(x):\n",
    "    return x.quantile(0.75) - x.quantile(0.25)\n",
    "\n",
    "def outlier_mask(x, sigma=3):\n",
    "    stdev = x.std()\n",
    "    upper = x.mean() + sigma*stdev\n",
    "    lower = x.mean() - sigma*stdev\n",
    "\n",
    "    return x.mask(((x > upper) | (x < lower)), np.nan).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validate_transform_pipeline(aq_file=AQ_FILE, wx_file=WX_FILE,\n",
    " diri=DIRI, verbose=True, interp=\"akima\"):\n",
    "    aq_file_path = os.path.join(diri, aq_file)\n",
    "    wx_file_path = os.path.join(diri, wx_file)\n",
    "\n",
    "    aq_df = pd.read_csv(aq_file_path)\n",
    "    wx_xr = xr.open_dataset(wx_file_path)\n",
    "    wx_df = xarray_conversion(wx_xr)\n",
    "\n",
    "    # Replace TOOL_ID numbers with more friendly integers\n",
    "    station_ids = df[\"TOOL_ID\"].unique()\n",
    "    df.replace(machine_ids, range(len(machine_ids)), inplace=True)\n",
    "\n",
    "    # All data is from SP5 in this dataset, remove column to strealine dataframe\n",
    "    df.drop(\"MODEL\", axis=1, errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Rename columns to more friendly names\n",
    "    df.columns = [\"time\", \"id\", \"variable\", \"value\"]\n",
    "\n",
    "    # Store and count unique variables in the \"variable\" column\n",
    "    variables = df[\"variable\"].unique()\n",
    "    N = len(variables)\n",
    "\n",
    "    # Store variable statistics in a dictionary\n",
    "    # Key: (Variable, Machine ID, Statistic) == number\n",
    "    # Statistics = [Mean, Standard Deviation, Skew Coefficient,\n",
    "    #               IQR, Outlier Count, Null Count, \n",
    "    #               Length of Dataset, Minimum, Maximum]\n",
    "    stats = df.groupby([\"variable\", \"id\"])[\"value\"].agg([np.mean, np.std,\n",
    "     sp.stats.skew, iqr, outlier_count, null_count, len, np.min, np.max])\n",
    "\n",
    "    if verbose:\n",
    "        print(stats)\n",
    "\n",
    "    stats.to_csv(f\"Output/{file}_STATISTICS.csv\")\n",
    "\n",
    "    '''\n",
    "    if compare:\n",
    "        compare input statistics against statistics of baseline/training data\n",
    "    \n",
    "    '''\n",
    "    # Calculate Z-Scores for Variable values with statistics from same machine\n",
    "    df[\"z\"] = df.groupby([\"variable\", \"id\"])[\"value\"].transform(zscore)\n",
    "\n",
    "    # Convert input's time to a pandas datetime\n",
    "    # df[\"datetime\"] = pd.to_datetime(df[\"time\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    # Transform data to daily median values\n",
    "    df_daily_median = pd.DataFrame(df.groupby([\"date\", \"variable\",\n",
    "     \"id\"])[\"z\"].agg(np.median))\n",
    "\n",
    "    df_daily_median = df_daily_median.reset_index(level=\"variable\")\n",
    "    df_daily_median = df_daily_median.pivot(columns=\"variable\").reset_index()\n",
    "\n",
    "    # Adjusting columns after pivoting\n",
    "    df_daily_median = df_daily_median.transpose().reset_index(level=0, drop=True)\n",
    "    df_daily_median.reset_index(inplace=True)\n",
    "    df_daily_median[\"variable\"][0] = \"date\"\n",
    "    df_daily_median[\"variable\"][1] = \"id\"\n",
    "\n",
    "    df_daily_median = df_daily_median.transpose()\n",
    "    df_daily_median.columns = df_daily_median.iloc[0]\n",
    "    df_daily_median.drop(\"variable\", axis=0, inplace=True)\n",
    "    df_daily_median.drop(\"alias\", axis=1, inplace=True)\n",
    "\n",
    "    # Date column may have change types from transposing\n",
    "    # Reset as index for akima spline\n",
    "    df_daily_median[\"date\"] = pd.to_datetime(df_daily_median[\"date\"])\n",
    "    df_daily_median = df_daily_median.set_index(\"date\")\n",
    "\n",
    "    # All data represent as objects after transposing\n",
    "    # Reset to float values\n",
    "    for var in df_daily_median.columns:\n",
    "        df_daily_median[var] = pd.to_numeric(df_daily_median[var],errors = 'coerce')\n",
    "\n",
    "    # Remove \"target\" variables\n",
    "    target_cols = [col for col in df_daily_median.columns if 'target' in col]\n",
    "    df_daily_median.drop(target_cols, axis=1, inplace=True)\n",
    "\n",
    "    # key = id\n",
    "    machine_dataframes = {}\n",
    "    for machine_id in range(len(machine_ids)):\n",
    "        temp_df = df_daily_median.loc[\n",
    "            df_daily_median[\"id\"] == machine_id]\n",
    "\n",
    "        # Set all outliers to Nan values\n",
    "        outlier_df = outlier_mask(temp_df)\n",
    "\n",
    "        # Interpolate Nan values using akima spline\n",
    "        machine_dataframes[machine_id] = outlier_df.interpolate(method=interp)\n",
    "        # machine_dataframes[machine_id] = outlier_df\n",
    "\n",
    "    return machine_dataframes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eeaf080965a2fa4526616a6825a7ae5e1aa772617450ce3e0570ac13d85c8ead"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
